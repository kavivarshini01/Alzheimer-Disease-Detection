app.py               


import streamlit as st
import os
import sqlite3
import pandas as pd
from src.user import predict, generate_heatmap, retrain_model

# Set up Streamlit app
st.title("ðŸ§  Alzheimer's Disease Detection")
st.write("Upload an MRI scan to classify its severity.")

# Ensure the temp directory exists
TEMP_DIR = "temp"
if not os.path.exists(TEMP_DIR):
    os.makedirs(TEMP_DIR)

# Upload MRI image
uploaded_file = st.file_uploader("Upload an MRI image", type=["jpg", "png", "jpeg"])

if uploaded_file is not None:
    # Save uploaded file to temp folder
    image_path = os.path.join(TEMP_DIR, uploaded_file.name)
    with open(image_path, "wb") as f:
        f.write(uploaded_file.getbuffer())

    st.image(image_path, caption="Uploaded MRI Scan", use_container_width=True) 
    # Perform prediction
    prediction, confidence = predict(image_path)
    st.write(f"### ðŸ¥ Prediction: **{prediction}**")
    st.write(f"ðŸ” Confidence Score: **{confidence * 100:.2f}%**")

    # Generate Grad-CAM Heatmap
    if st.button("Generate Heatmap"):
        heatmap_path = generate_heatmap(image_path)
        st.image(heatmap_path, caption="Grad-CAM Heatmap", use_container_width=300) 


# Retrain model button
if st.button("Retrain Model"):
    with st.spinner("Retraining in progress..."):
        retrain_model()
    st.success("âœ… Model retrained successfully!")

# View patient history from database
st.write("### ðŸ“œ Patient History")

def fetch_patient_history():
    conn = sqlite3.connect("patient_history.db")
    df = pd.read_sql_query("SELECT * FROM history ORDER BY id DESC", conn)
    conn.close()
    return df

history_df = fetch_patient_history()
if not history_df.empty:
    st.dataframe(history_df)
else:
    st.write("No patient history available.")


database.py

import sqlite3

# Connect to SQLite database (creates file if not exists)
conn = sqlite3.connect("patient_history.db")
cursor = conn.cursor()

# Create table if it doesn't exist
cursor.execute('''
    CREATE TABLE IF NOT EXISTS predictions (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        image_path TEXT,
        prediction TEXT,
        confidence REAL
    )
''')

conn.commit()
conn.close()


user.py

import os
import torch
import torchvision.transforms as transforms
from PIL import Image
import numpy as np
import joblib
import torch.nn as nn
import cv2
import sqlite3

# Load trained models
cnn_model_path = "models/cnn_model.pth"
svm_model_path = "models/svm_model.pkl"
scaler_path = "models/scaler.pkl"
label_encoder_path = "models/label_encoder.pkl"

# Define CNN Model
class CNNModel(nn.Module):
    def __init__(self):
        super(CNNModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(64 * 32 * 32, 128)
        self.fc2 = nn.Linear(128, 4)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))
        x = self.pool(self.relu(self.conv2(x)))
        x = self.flatten(x)
        x = self.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Load models
cnn_model = CNNModel()
cnn_model.load_state_dict(torch.load(cnn_model_path, map_location=torch.device("cpu")))
cnn_model.eval()

svm_model = joblib.load(svm_model_path)
scaler = joblib.load(scaler_path)
label_encoder = joblib.load(label_encoder_path)

# SQLite database setup
DB_PATH = "patient_history.db"

def init_db():
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute('''CREATE TABLE IF NOT EXISTS history
                 (id INTEGER PRIMARY KEY AUTOINCREMENT,
                 image_path TEXT, prediction TEXT, confidence REAL)''')
    conn.commit()
    conn.close()

init_db()

def save_to_db(image_path, prediction, confidence):
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("INSERT INTO history (image_path, prediction, confidence) VALUES (?, ?, ?)", 
              (image_path, prediction, confidence))
    conn.commit()
    conn.close()

# Image Preprocessing
def preprocess_image(image_path):
    image = Image.open(image_path).convert("RGB")
    transform = transforms.Compose([
        transforms.Resize((128, 128)),
        transforms.ToTensor()
    ])
    return transform(image).unsqueeze(0)

# Prediction Function
def predict(image_path):
    image_tensor = preprocess_image(image_path)

    # Extract CNN feature vector
    with torch.no_grad():
        cnn_features = cnn_model(image_tensor).numpy()

    # Normalize features before SVM prediction
    cnn_features = scaler.transform(cnn_features)

    # Predict with SVM
    prediction = svm_model.predict(cnn_features)[0]
    confidence = np.max(svm_model.predict_proba(cnn_features))

    predicted_label = label_encoder.inverse_transform([prediction])[0]
    
    # Save to database
    save_to_db(image_path, predicted_label, confidence)
    
    return predicted_label, confidence

# Grad-CAM Heatmap Function
def generate_heatmap(image_path):
    image_tensor = preprocess_image(image_path)
    image_tensor.requires_grad_()

    # Forward pass
    output = cnn_model(image_tensor)
    class_idx = output.argmax().item()

    # Backpropagation
    cnn_model.zero_grad()
    output[0, class_idx].backward()

    # Extract gradients from the last convolutional layer
    gradients = cnn_model.conv2.weight.grad.mean(dim=[0, 2, 3]).detach().numpy()

    # Convert image tensor to numpy for visualization
    image_np = image_tensor.detach().squeeze().permute(1, 2, 0).numpy()

    # Normalize and apply heatmap
    heatmap = np.maximum(gradients, 0)
    heatmap = cv2.resize(heatmap, (128, 128))

    if heatmap.max() > heatmap.min():  
        heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())

    heatmap = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)

    # Overlay heatmap on the image
    overlay = cv2.addWeighted(cv2.cvtColor((image_np * 255).astype(np.uint8), cv2.COLOR_RGB2BGR), 0.6, heatmap, 0.4, 0)

    # Save and return heatmap
    heatmap_path = "heatmap.jpg"
    cv2.imwrite(heatmap_path, overlay)
    return heatmap_path

# Retrain Model Function
def retrain_model():
    os.system("python src/train.py")


model.py

import torch
import torch.nn as nn
import torch.nn.functional as F

# Define CNN Model
class CNNModel(nn.Module):
    def __init__(self):
        super(CNNModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(64 * 32 * 32, 128)
        self.fc2 = nn.Linear(128, 4)  # 4 classes (Mild, Very Mild, Moderate, Demented)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = self.flatten(x)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x


train.py


import os
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import numpy as np
import joblib
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, Subset

# Define CNN Model
class CNNModel(nn.Module):
    def __init__(self):
        super(CNNModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(64 * 32 * 32, 128)
        self.fc2 = nn.Linear(128, 4)  # 4 output classes
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))
        x = self.pool(self.relu(self.conv2(x)))
        x = self.flatten(x)
        x = self.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Dataset Preparation
transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor()
])

dataset_path = "D:\\PROJECT\\Alzheimer\\dataset"
dataset = datasets.ImageFolder(root=dataset_path, transform=transform)

# Apply Label Encoding for class labels
label_encoder = LabelEncoder()
encoded_labels = label_encoder.fit_transform([dataset.classes[i] for i in dataset.targets])

# Split dataset
train_indices, test_indices = train_test_split(np.arange(len(dataset)), test_size=0.2, stratify=encoded_labels, random_state=42)
train_dataset = Subset(dataset, train_indices)
test_dataset = Subset(dataset, test_indices)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

print("Dataset uploaded properly")

# Train CNN model
model = CNNModel()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

print("Training CNN model...")

def train_cnn():
    model.train()
    for epoch in range(10):  
        for images, labels in train_loader:
            images = images.to(torch.float32)  # Ensure correct dtype
            labels = labels.to(torch.long)  

            optimizer.zero_grad()
            outputs = model(images)  
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
        print(f"Epoch {epoch+1}, Loss: {loss.item()}")
    torch.save(model.state_dict(), "models/cnn_model.pth")

# Extract CNN Features & Train SVM
def extract_features():
    model.eval()
    features = []
    labels = []
    with torch.no_grad():
        for images, lbls in train_loader:
            images = images.to(torch.float32)
            outputs = model(images)
            features.extend(outputs.numpy())
            labels.extend(lbls.numpy())
    return np.array(features), np.array(labels)

def train_svm():
    features, labels = extract_features()
    
    # Normalize features
    scaler = StandardScaler()
    features = scaler.fit_transform(features)

    svm = SVC(kernel='linear', probability=True)
    svm.fit(features, labels)

    # Save models & encoders
    joblib.dump(svm, "models/svm_model.pkl")
    joblib.dump(scaler, "models/scaler.pkl")
    joblib.dump(label_encoder, "models/label_encoder.pkl")

if __name__ == "__main__":
    train_cnn()
    print("CNN Training Completed")

    print("Training SVM...")
    train_svm()
    print("Training Complete!")









